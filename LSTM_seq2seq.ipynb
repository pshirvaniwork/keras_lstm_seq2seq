{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "LSTM_seq2seq.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pshirvaniwork/keras_lstm_seq2seq/blob/master/LSTM_seq2seq.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Mg36wNwA-zYJ"
      },
      "source": [
        "\n",
        "# Sequence to sequence example in Keras (character-level).\n",
        "This script demonstrates how to implement a basic character-level\n",
        "sequence-to-sequence model. We apply it to translating\n",
        "short English sentences into short French sentences,\n",
        "character-by-character. Note that it is fairly unusual to\n",
        "do character-level machine translation, as word-level\n",
        "models are more common in this domain.\n",
        "\n",
        "**Summary of the algorithm**\n",
        "- We start with input sequences from a domain (e.g. English sentences)\n",
        "    and corresponding target sequences from another domain\n",
        "    (e.g. French sentences).\n",
        "- An encoder LSTM turns input sequences to 2 state vectors\n",
        "    (we keep the last LSTM state and discard the outputs).\n",
        "- A decoder LSTM is trained to turn the target sequences into\n",
        "    the same sequence but offset by one timestep in the future,\n",
        "    a training process called \"teacher forcing\" in this context.\n",
        "    It uses as initial state the state vectors from the encoder.\n",
        "    Effectively, the decoder learns to generate `targets[t+1...]`\n",
        "    given `targets[...t]`, conditioned on the input sequence.\n",
        "- In inference mode, when we want to decode unknown input sequences, we:\n",
        "    - Encode the input sequence into state vectors\n",
        "    - Start with a target sequence of size 1\n",
        "        (just the start-of-sequence character)\n",
        "    - Feed the state vectors and 1-char target sequence\n",
        "        to the decoder to produce predictions for the next character\n",
        "    - Sample the next character using these predictions\n",
        "        (we simply use argmax).\n",
        "    - Append the sampled character to the target sequence\n",
        "    - Repeat until we generate the end-of-sequence character or we\n",
        "        hit the character limit.\n",
        "**Data download** :=\n",
        "\n",
        "  - [English to French sentence pairs.](http://www.manythings.org/anki/fra-eng.zip)\n",
        "  -[Lots of neat sentence pairs datasets.\n",
        "](http://www.manythings.org/anki/)\n",
        "\n",
        "**References**:\n",
        "  - [Sequence to Sequence Learning with Neural Networks\n",
        "   ](https://arxiv.org/abs/1409.3215)\n",
        "  - [Learning Phrase Representations using\n",
        "    RNN Encoder-Decoder for Statistical Machine Translation\n",
        "    ](https://arxiv.org/abs/1406.1078)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ZfG6r7bC-d1",
        "colab_type": "text"
      },
      "source": [
        "## Load Python packages "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "nSVsGsj7-7Tm",
        "outputId": "b47fffa2-1124-4939-c488-5c4710c2262e",
        "scrolled": false,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81
        }
      },
      "source": [
        "from __future__ import print_function\n",
        "from keras.models import Model\n",
        "from keras.layers import Input, LSTM, Dense\n",
        "import numpy as np\n",
        "import os\n",
        "import yaml\n",
        "import requests, zipfile, io\n",
        "from urllib.request import urlopen\n",
        "from zipfile import ZipFile"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2X8AGRd2C-d6",
        "colab_type": "text"
      },
      "source": [
        "## Setup variables\n",
        "\n",
        "We will first setup model varibales and data path.<br>\n",
        "1. Batch size for training\n",
        "2. Number of epochs to train for\n",
        "3. Latent dimensionality of the encoding space\n",
        "4. Number of samples to train on\n",
        "5. Path to training data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m_pa-K-jC-d7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch_size = 128  # Batch size for training.\n",
        "epochs = 100  # Number of epochs to train for.\n",
        "latent_dim = 256  # Latent dimensionality of the encoding space.\n",
        "num_samples = 10000  # Number of samples to train on.\n",
        "# Path to the data txt file on disk.\n",
        "data_path = 'fra.txt'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NCWQTsP1Dfqt",
        "colab_type": "text"
      },
      "source": [
        "## Load dataset\n",
        " We need data to train our translation model. We will be using English to French traning data for this exercise. Data can be downloded from the following GitHub link:\n",
        "https://github.com/pshirvaniwork/keras_lstm_seq2seq/blob/master/fra-eng.zip?raw=True\n",
        "\n",
        "Example:<br>\n",
        "Go on.  Poursuis.       CC-BY 2.0 (France) Attribution: tatoeba.org #2230774 (CK) & #588132 (sacredceltic)<br>\n",
        "Go on.  Continuez.      CC-BY 2.0 (France) Attribution: tatoeba.org #2230774 (CK) & #6463161 (Aiji)<br>\n",
        "Go on.  Poursuivez.     CC-BY 2.0 (France) Attribution: tatoeba.org #2230774 (CK) & #6463162 (Aiji)<br>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "KbKt-Dtr_sRJ",
        "colab": {}
      },
      "source": [
        "zipurl = 'https://github.com/pshirvaniwork/keras_lstm_seq2seq/blob/master/fra-eng.zip?raw=True'\n",
        "# Download the file from the URL\n",
        "r = requests.get(zipurl)\n",
        "z = zipfile.ZipFile(io.BytesIO(r.content))\n",
        "z.extractall()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "SHi-sglel5-2"
      },
      "source": [
        "## Read the training dataset file\n",
        "\n",
        "1. Create input and target texts\n",
        "2. Create input and target character set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "LX1aDT6xKIIR",
        "colab": {}
      },
      "source": [
        "input_texts = []\n",
        "target_texts = []\n",
        "input_characters = set()\n",
        "target_characters = set()\n",
        "with open(os.getcwd()+ os.sep + data_path, 'r', encoding='utf-8') as f:\n",
        "    lines = f.read().split('\\n')\n",
        "for line in lines[: min(num_samples, len(lines) - 1)]:\n",
        "    input_text, target_text, _ = line.split('\\t')\n",
        "    # We use \"tab\" as the \"start sequence\" character\n",
        "    # for the targets, and \"\\n\" as \"end sequence\" character.\n",
        "    target_text = '\\t' + target_text + '\\n'\n",
        "    input_texts.append(input_text)\n",
        "    target_texts.append(target_text)\n",
        "    for char in input_text:\n",
        "        if char not in input_characters:\n",
        "            input_characters.add(char)\n",
        "    for char in target_text:\n",
        "        if char not in target_characters:\n",
        "            target_characters.add(char)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HScaLtwWC-eM",
        "colab_type": "text"
      },
      "source": [
        "## Lets have a look"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "GIK1RUYCNxpQ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "outputId": "11d3cdb4-2108-444b-c671-b551cf063eed"
      },
      "source": [
        "input_characters = sorted(list(input_characters))\n",
        "target_characters = sorted(list(target_characters))\n",
        "num_encoder_tokens = len(input_characters)\n",
        "num_decoder_tokens = len(target_characters)\n",
        "max_encoder_seq_length = max([len(txt) for txt in input_texts])\n",
        "max_decoder_seq_length = max([len(txt) for txt in target_texts])\n",
        "print('Number of samples:', len(input_texts))\n",
        "print('Number of unique input tokens:', num_encoder_tokens)\n",
        "print('Number of unique output tokens:', num_decoder_tokens)\n",
        "print('Max sequence length for inputs:', max_encoder_seq_length)\n",
        "print('Max sequence length for outputs:', max_decoder_seq_length)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of samples: 10000\n",
            "Number of unique input tokens: 70\n",
            "Number of unique output tokens: 93\n",
            "Max sequence length for inputs: 16\n",
            "Max sequence length for outputs: 59\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "IdpgrrwVghvm"
      },
      "source": [
        "## Turn the sentences into 3D Numpy arrays\n",
        "1. encoder_input_data is a 3D array of shape (num_pairs, max_english_sentence_length, num_english_characters) containing a one-hot vectorization of the English sentences.\n",
        "2. decoder_input_data is a 3D array of shape (num_pairs, max_french_sentence_length, num_french_characters) containg a one-hot vectorization of the French sentences.\n",
        "3. decoder_target_data is the same as decoder_input_data but offset by one timestep. decoder_target_data[:, t, :] will be the same as decoder_input_data[:, t + 1, :]."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "8zVM0NSxOJh9",
        "colab": {}
      },
      "source": [
        "# Create character index dictionary\n",
        "input_token_index = dict(\n",
        "    [(char, i) for i, char in enumerate(input_characters)])\n",
        "target_token_index = dict(\n",
        "    [(char, i) for i, char in enumerate(target_characters)])\n",
        "\n",
        "# Setup matrices for encoder and decoder\n",
        "encoder_input_data = np.zeros(\n",
        "    (len(input_texts), max_encoder_seq_length, num_encoder_tokens),\n",
        "    dtype='float32')\n",
        "decoder_input_data = np.zeros(\n",
        "    (len(input_texts), max_decoder_seq_length, num_decoder_tokens),\n",
        "    dtype='float32')\n",
        "decoder_target_data = np.zeros(\n",
        "    (len(input_texts), max_decoder_seq_length, num_decoder_tokens),\n",
        "    dtype='float32')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "i0OfrPQEOOMP",
        "colab": {}
      },
      "source": [
        "for i, (input_text, target_text) in enumerate(zip(input_texts, target_texts)):\n",
        "    for t, char in enumerate(input_text):\n",
        "        encoder_input_data[i, t, input_token_index[char]] = 1.\n",
        "    encoder_input_data[i, t + 1:, input_token_index[' ']] = 1.\n",
        "    for t, char in enumerate(target_text):\n",
        "        # decoder_target_data is ahead of decoder_input_data by one timestep\n",
        "        decoder_input_data[i, t, target_token_index[char]] = 1.\n",
        "        if t > 0:\n",
        "            # decoder_target_data will be ahead by one timestep\n",
        "            # and will not include the start character.\n",
        "            decoder_target_data[i, t - 1, target_token_index[char]] = 1.\n",
        "    decoder_input_data[i, t + 1:, target_token_index[' ']] = 1.\n",
        "    decoder_target_data[i, t:, target_token_index[' ']] = 1.\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "0iOpMnAch3jH"
      },
      "source": [
        "## Define a basic LSTM-based Seq2Seq model \n",
        "1. Define an input sequence and process it\n",
        "2. Discard `encoder_outputs` and only keep the states. \n",
        "3. Set up the decoder, using `encoder_states` as initial state.\n",
        "4.  We set up our decoder to return full output sequences, and to return internal states as well"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "VzyIWQ5XOTo4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "outputId": "b8d8f7dd-e64d-4b8c-8485-e878345aea45"
      },
      "source": [
        "# Define an input sequence and process it.\n",
        "encoder_inputs = Input(shape=(None, num_encoder_tokens))\n",
        "encoder = LSTM(latent_dim, return_state=True)\n",
        "encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
        "# We discard `encoder_outputs` and only keep the states.\n",
        "encoder_states = [state_h, state_c]\n",
        "# Set up the decoder, using `encoder_states` as initial state.\n",
        "decoder_inputs = Input(shape=(None, num_decoder_tokens))\n",
        "# We set up our decoder to return full output sequences,\n",
        "# and to return internal states as well. We don't use the\n",
        "# return states in the training model, but we will use them in inference.\n",
        "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
        "decoder_outputs, _, _ = decoder_lstm(decoder_inputs,\n",
        "                                     initial_state=encoder_states)\n",
        "decoder_dense = Dense(num_decoder_tokens, activation='softmax')\n",
        "decoder_outputs = decoder_dense(decoder_outputs)\n",
        "\n",
        "# Define the model that will turn\n",
        "# `encoder_input_data` & `decoder_input_data` into `decoder_target_data`\n",
        "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L1yNUSdCOMJr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "5q2BdD66luYW"
      },
      "source": [
        "## Training time"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "T6ulb4MSlkHe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "3f7a8f9b-453d-40f8-f329-dc2f23bc90be"
      },
      "source": [
        "# Run training\n",
        "model.compile(optimizer='rmsprop', loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "model.fit([encoder_input_data, decoder_input_data], decoder_target_data,\n",
        "          batch_size=batch_size,\n",
        "          epochs=epochs,\n",
        "          validation_split=0.2)\n",
        "# Save model\n",
        "model.save('s2s.h5')"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3576: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3005: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "Train on 8000 samples, validate on 2000 samples\n",
            "Epoch 1/100\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "8000/8000 [==============================] - 11s 1ms/step - loss: 1.3958 - acc: 0.7123 - val_loss: 1.2474 - val_acc: 0.6974\n",
            "Epoch 2/100\n",
            "8000/8000 [==============================] - 6s 800us/step - loss: 1.0150 - acc: 0.7341 - val_loss: 1.0718 - val_acc: 0.7104\n",
            "Epoch 3/100\n",
            "8000/8000 [==============================] - 7s 827us/step - loss: 0.9002 - acc: 0.7555 - val_loss: 0.9717 - val_acc: 0.7456\n",
            "Epoch 4/100\n",
            "8000/8000 [==============================] - 7s 840us/step - loss: 0.7762 - acc: 0.7878 - val_loss: 0.8333 - val_acc: 0.7689\n",
            "Epoch 5/100\n",
            "8000/8000 [==============================] - 7s 849us/step - loss: 0.7002 - acc: 0.8059 - val_loss: 0.7615 - val_acc: 0.7823\n",
            "Epoch 6/100\n",
            "8000/8000 [==============================] - 6s 803us/step - loss: 0.6319 - acc: 0.8178 - val_loss: 0.7253 - val_acc: 0.7925\n",
            "Epoch 7/100\n",
            "8000/8000 [==============================] - 6s 767us/step - loss: 0.5899 - acc: 0.8280 - val_loss: 0.6744 - val_acc: 0.8021\n",
            "Epoch 8/100\n",
            "8000/8000 [==============================] - 7s 850us/step - loss: 0.5574 - acc: 0.8366 - val_loss: 0.6363 - val_acc: 0.8127\n",
            "Epoch 9/100\n",
            "8000/8000 [==============================] - 6s 785us/step - loss: 0.5299 - acc: 0.8444 - val_loss: 0.6103 - val_acc: 0.8196\n",
            "Epoch 10/100\n",
            "8000/8000 [==============================] - 6s 797us/step - loss: 0.5078 - acc: 0.8507 - val_loss: 0.5955 - val_acc: 0.8252\n",
            "Epoch 11/100\n",
            "8000/8000 [==============================] - 6s 787us/step - loss: 0.4884 - acc: 0.8557 - val_loss: 0.5762 - val_acc: 0.8297\n",
            "Epoch 12/100\n",
            "8000/8000 [==============================] - 6s 771us/step - loss: 0.4711 - acc: 0.8608 - val_loss: 0.5624 - val_acc: 0.8338\n",
            "Epoch 13/100\n",
            "8000/8000 [==============================] - 6s 766us/step - loss: 0.4548 - acc: 0.8646 - val_loss: 0.5528 - val_acc: 0.8383\n",
            "Epoch 14/100\n",
            "8000/8000 [==============================] - 7s 829us/step - loss: 0.4400 - acc: 0.8690 - val_loss: 0.5317 - val_acc: 0.8416\n",
            "Epoch 15/100\n",
            "8000/8000 [==============================] - 6s 752us/step - loss: 0.4265 - acc: 0.8723 - val_loss: 0.5238 - val_acc: 0.8427\n",
            "Epoch 16/100\n",
            "8000/8000 [==============================] - 6s 799us/step - loss: 0.4140 - acc: 0.8759 - val_loss: 0.5164 - val_acc: 0.8456\n",
            "Epoch 17/100\n",
            "8000/8000 [==============================] - 6s 810us/step - loss: 0.4019 - acc: 0.8794 - val_loss: 0.5105 - val_acc: 0.8477\n",
            "Epoch 18/100\n",
            "8000/8000 [==============================] - 6s 802us/step - loss: 0.3902 - acc: 0.8826 - val_loss: 0.5046 - val_acc: 0.8504\n",
            "Epoch 19/100\n",
            "8000/8000 [==============================] - 6s 798us/step - loss: 0.3802 - acc: 0.8858 - val_loss: 0.4937 - val_acc: 0.8534\n",
            "Epoch 20/100\n",
            "8000/8000 [==============================] - 7s 834us/step - loss: 0.3694 - acc: 0.8887 - val_loss: 0.4846 - val_acc: 0.8562\n",
            "Epoch 21/100\n",
            "8000/8000 [==============================] - 6s 770us/step - loss: 0.3600 - acc: 0.8914 - val_loss: 0.4810 - val_acc: 0.8576\n",
            "Epoch 22/100\n",
            "8000/8000 [==============================] - 6s 767us/step - loss: 0.3499 - acc: 0.8942 - val_loss: 0.4765 - val_acc: 0.8592\n",
            "Epoch 23/100\n",
            "8000/8000 [==============================] - 7s 821us/step - loss: 0.3413 - acc: 0.8969 - val_loss: 0.4699 - val_acc: 0.8617\n",
            "Epoch 24/100\n",
            "8000/8000 [==============================] - 6s 789us/step - loss: 0.3327 - acc: 0.8998 - val_loss: 0.4682 - val_acc: 0.8625\n",
            "Epoch 25/100\n",
            "8000/8000 [==============================] - 7s 817us/step - loss: 0.3240 - acc: 0.9022 - val_loss: 0.4703 - val_acc: 0.8616\n",
            "Epoch 26/100\n",
            "8000/8000 [==============================] - 7s 847us/step - loss: 0.3162 - acc: 0.9043 - val_loss: 0.4676 - val_acc: 0.8624\n",
            "Epoch 27/100\n",
            "8000/8000 [==============================] - 6s 765us/step - loss: 0.3080 - acc: 0.9069 - val_loss: 0.4672 - val_acc: 0.8638\n",
            "Epoch 28/100\n",
            "8000/8000 [==============================] - 7s 876us/step - loss: 0.3009 - acc: 0.9089 - val_loss: 0.4602 - val_acc: 0.8647\n",
            "Epoch 29/100\n",
            "8000/8000 [==============================] - 6s 776us/step - loss: 0.2932 - acc: 0.9117 - val_loss: 0.4570 - val_acc: 0.8671\n",
            "Epoch 30/100\n",
            "8000/8000 [==============================] - 6s 794us/step - loss: 0.2864 - acc: 0.9133 - val_loss: 0.4575 - val_acc: 0.8666\n",
            "Epoch 31/100\n",
            "8000/8000 [==============================] - 6s 801us/step - loss: 0.2797 - acc: 0.9155 - val_loss: 0.4578 - val_acc: 0.8671\n",
            "Epoch 32/100\n",
            "8000/8000 [==============================] - 7s 823us/step - loss: 0.2726 - acc: 0.9173 - val_loss: 0.4634 - val_acc: 0.8669\n",
            "Epoch 33/100\n",
            "8000/8000 [==============================] - 6s 809us/step - loss: 0.2663 - acc: 0.9196 - val_loss: 0.4598 - val_acc: 0.8674\n",
            "Epoch 34/100\n",
            "8000/8000 [==============================] - 7s 842us/step - loss: 0.2600 - acc: 0.9212 - val_loss: 0.4616 - val_acc: 0.8676\n",
            "Epoch 35/100\n",
            "8000/8000 [==============================] - 6s 810us/step - loss: 0.2543 - acc: 0.9230 - val_loss: 0.4573 - val_acc: 0.8695\n",
            "Epoch 36/100\n",
            "8000/8000 [==============================] - 6s 796us/step - loss: 0.2484 - acc: 0.9249 - val_loss: 0.4611 - val_acc: 0.8694\n",
            "Epoch 37/100\n",
            "8000/8000 [==============================] - 7s 816us/step - loss: 0.2425 - acc: 0.9263 - val_loss: 0.4639 - val_acc: 0.8683\n",
            "Epoch 38/100\n",
            "8000/8000 [==============================] - 6s 767us/step - loss: 0.2372 - acc: 0.9279 - val_loss: 0.4719 - val_acc: 0.8680\n",
            "Epoch 39/100\n",
            "8000/8000 [==============================] - 7s 829us/step - loss: 0.2317 - acc: 0.9295 - val_loss: 0.4662 - val_acc: 0.8691\n",
            "Epoch 40/100\n",
            "8000/8000 [==============================] - 6s 806us/step - loss: 0.2271 - acc: 0.9308 - val_loss: 0.4679 - val_acc: 0.8695\n",
            "Epoch 41/100\n",
            "8000/8000 [==============================] - 7s 853us/step - loss: 0.2218 - acc: 0.9323 - val_loss: 0.4638 - val_acc: 0.8705\n",
            "Epoch 42/100\n",
            "8000/8000 [==============================] - 6s 767us/step - loss: 0.2169 - acc: 0.9339 - val_loss: 0.4715 - val_acc: 0.8690\n",
            "Epoch 43/100\n",
            "8000/8000 [==============================] - 7s 814us/step - loss: 0.2122 - acc: 0.9355 - val_loss: 0.4774 - val_acc: 0.8697\n",
            "Epoch 44/100\n",
            "8000/8000 [==============================] - 6s 792us/step - loss: 0.2072 - acc: 0.9372 - val_loss: 0.4886 - val_acc: 0.8680\n",
            "Epoch 45/100\n",
            "8000/8000 [==============================] - 7s 841us/step - loss: 0.2027 - acc: 0.9383 - val_loss: 0.4855 - val_acc: 0.8689\n",
            "Epoch 46/100\n",
            "8000/8000 [==============================] - 6s 802us/step - loss: 0.1986 - acc: 0.9396 - val_loss: 0.4754 - val_acc: 0.8701\n",
            "Epoch 47/100\n",
            "8000/8000 [==============================] - 6s 775us/step - loss: 0.1939 - acc: 0.9409 - val_loss: 0.4805 - val_acc: 0.8702\n",
            "Epoch 48/100\n",
            "8000/8000 [==============================] - 6s 753us/step - loss: 0.1903 - acc: 0.9420 - val_loss: 0.4812 - val_acc: 0.8707\n",
            "Epoch 49/100\n",
            "8000/8000 [==============================] - 7s 815us/step - loss: 0.1862 - acc: 0.9434 - val_loss: 0.4898 - val_acc: 0.8701\n",
            "Epoch 50/100\n",
            "8000/8000 [==============================] - 7s 817us/step - loss: 0.1821 - acc: 0.9446 - val_loss: 0.4917 - val_acc: 0.8700\n",
            "Epoch 51/100\n",
            "8000/8000 [==============================] - 7s 824us/step - loss: 0.1781 - acc: 0.9458 - val_loss: 0.4946 - val_acc: 0.8706\n",
            "Epoch 52/100\n",
            "8000/8000 [==============================] - 6s 788us/step - loss: 0.1747 - acc: 0.9465 - val_loss: 0.5041 - val_acc: 0.8691\n",
            "Epoch 53/100\n",
            "8000/8000 [==============================] - 7s 822us/step - loss: 0.1712 - acc: 0.9478 - val_loss: 0.5063 - val_acc: 0.8691\n",
            "Epoch 54/100\n",
            "8000/8000 [==============================] - 6s 803us/step - loss: 0.1679 - acc: 0.9487 - val_loss: 0.5070 - val_acc: 0.8698\n",
            "Epoch 55/100\n",
            "8000/8000 [==============================] - 6s 776us/step - loss: 0.1643 - acc: 0.9499 - val_loss: 0.5117 - val_acc: 0.8687\n",
            "Epoch 56/100\n",
            "8000/8000 [==============================] - 7s 815us/step - loss: 0.1610 - acc: 0.9506 - val_loss: 0.5127 - val_acc: 0.8704\n",
            "Epoch 57/100\n",
            "8000/8000 [==============================] - 7s 864us/step - loss: 0.1579 - acc: 0.9517 - val_loss: 0.5105 - val_acc: 0.8705\n",
            "Epoch 58/100\n",
            "8000/8000 [==============================] - 6s 760us/step - loss: 0.1548 - acc: 0.9526 - val_loss: 0.5133 - val_acc: 0.8704\n",
            "Epoch 59/100\n",
            "8000/8000 [==============================] - 7s 813us/step - loss: 0.1513 - acc: 0.9538 - val_loss: 0.5222 - val_acc: 0.8704\n",
            "Epoch 60/100\n",
            "8000/8000 [==============================] - 6s 809us/step - loss: 0.1492 - acc: 0.9543 - val_loss: 0.5239 - val_acc: 0.8709\n",
            "Epoch 61/100\n",
            "8000/8000 [==============================] - 6s 787us/step - loss: 0.1455 - acc: 0.9552 - val_loss: 0.5255 - val_acc: 0.8703\n",
            "Epoch 62/100\n",
            "8000/8000 [==============================] - 6s 784us/step - loss: 0.1428 - acc: 0.9561 - val_loss: 0.5279 - val_acc: 0.8712\n",
            "Epoch 63/100\n",
            "8000/8000 [==============================] - 6s 805us/step - loss: 0.1401 - acc: 0.9571 - val_loss: 0.5364 - val_acc: 0.8703\n",
            "Epoch 64/100\n",
            "8000/8000 [==============================] - 6s 762us/step - loss: 0.1376 - acc: 0.9578 - val_loss: 0.5370 - val_acc: 0.8706\n",
            "Epoch 65/100\n",
            "8000/8000 [==============================] - 7s 848us/step - loss: 0.1351 - acc: 0.9584 - val_loss: 0.5438 - val_acc: 0.8700\n",
            "Epoch 66/100\n",
            "8000/8000 [==============================] - 7s 827us/step - loss: 0.1326 - acc: 0.9592 - val_loss: 0.5422 - val_acc: 0.8703\n",
            "Epoch 67/100\n",
            "8000/8000 [==============================] - 6s 809us/step - loss: 0.1296 - acc: 0.9602 - val_loss: 0.5564 - val_acc: 0.8691\n",
            "Epoch 68/100\n",
            "8000/8000 [==============================] - 7s 831us/step - loss: 0.1278 - acc: 0.9605 - val_loss: 0.5506 - val_acc: 0.8701\n",
            "Epoch 69/100\n",
            "8000/8000 [==============================] - 7s 823us/step - loss: 0.1252 - acc: 0.9612 - val_loss: 0.5554 - val_acc: 0.8699\n",
            "Epoch 70/100\n",
            "8000/8000 [==============================] - 6s 794us/step - loss: 0.1230 - acc: 0.9621 - val_loss: 0.5637 - val_acc: 0.8688\n",
            "Epoch 71/100\n",
            "8000/8000 [==============================] - 6s 805us/step - loss: 0.1207 - acc: 0.9630 - val_loss: 0.5633 - val_acc: 0.8701\n",
            "Epoch 72/100\n",
            "8000/8000 [==============================] - 7s 818us/step - loss: 0.1183 - acc: 0.9633 - val_loss: 0.5660 - val_acc: 0.8698\n",
            "Epoch 73/100\n",
            "8000/8000 [==============================] - 7s 824us/step - loss: 0.1165 - acc: 0.9640 - val_loss: 0.5716 - val_acc: 0.8689\n",
            "Epoch 74/100\n",
            "8000/8000 [==============================] - 6s 778us/step - loss: 0.1145 - acc: 0.9644 - val_loss: 0.5727 - val_acc: 0.8693\n",
            "Epoch 75/100\n",
            "8000/8000 [==============================] - 6s 776us/step - loss: 0.1120 - acc: 0.9653 - val_loss: 0.5789 - val_acc: 0.8695\n",
            "Epoch 76/100\n",
            "8000/8000 [==============================] - 6s 783us/step - loss: 0.1104 - acc: 0.9658 - val_loss: 0.5939 - val_acc: 0.8670\n",
            "Epoch 77/100\n",
            "8000/8000 [==============================] - 6s 811us/step - loss: 0.1087 - acc: 0.9664 - val_loss: 0.5910 - val_acc: 0.8691\n",
            "Epoch 78/100\n",
            "8000/8000 [==============================] - 6s 755us/step - loss: 0.1066 - acc: 0.9669 - val_loss: 0.5913 - val_acc: 0.8688\n",
            "Epoch 79/100\n",
            "8000/8000 [==============================] - 6s 805us/step - loss: 0.1047 - acc: 0.9674 - val_loss: 0.5908 - val_acc: 0.8683\n",
            "Epoch 80/100\n",
            "8000/8000 [==============================] - 6s 799us/step - loss: 0.1030 - acc: 0.9680 - val_loss: 0.5989 - val_acc: 0.8684\n",
            "Epoch 81/100\n",
            "8000/8000 [==============================] - 7s 826us/step - loss: 0.1009 - acc: 0.9685 - val_loss: 0.6090 - val_acc: 0.8682\n",
            "Epoch 82/100\n",
            "8000/8000 [==============================] - 7s 838us/step - loss: 0.0996 - acc: 0.9688 - val_loss: 0.6037 - val_acc: 0.8695\n",
            "Epoch 83/100\n",
            "8000/8000 [==============================] - 6s 775us/step - loss: 0.0977 - acc: 0.9696 - val_loss: 0.6136 - val_acc: 0.8681\n",
            "Epoch 84/100\n",
            "8000/8000 [==============================] - 6s 785us/step - loss: 0.0962 - acc: 0.9699 - val_loss: 0.6148 - val_acc: 0.8685\n",
            "Epoch 85/100\n",
            "8000/8000 [==============================] - 6s 809us/step - loss: 0.0942 - acc: 0.9705 - val_loss: 0.6190 - val_acc: 0.8689\n",
            "Epoch 86/100\n",
            "8000/8000 [==============================] - 7s 822us/step - loss: 0.0930 - acc: 0.9707 - val_loss: 0.6221 - val_acc: 0.8681\n",
            "Epoch 87/100\n",
            "8000/8000 [==============================] - 6s 776us/step - loss: 0.0915 - acc: 0.9713 - val_loss: 0.6186 - val_acc: 0.8692\n",
            "Epoch 88/100\n",
            "8000/8000 [==============================] - 7s 843us/step - loss: 0.0896 - acc: 0.9718 - val_loss: 0.6298 - val_acc: 0.8682\n",
            "Epoch 89/100\n",
            "8000/8000 [==============================] - 6s 767us/step - loss: 0.0885 - acc: 0.9720 - val_loss: 0.6281 - val_acc: 0.8681\n",
            "Epoch 90/100\n",
            "8000/8000 [==============================] - 7s 817us/step - loss: 0.0872 - acc: 0.9724 - val_loss: 0.6289 - val_acc: 0.8692\n",
            "Epoch 91/100\n",
            "8000/8000 [==============================] - 6s 789us/step - loss: 0.0850 - acc: 0.9732 - val_loss: 0.6433 - val_acc: 0.8672\n",
            "Epoch 92/100\n",
            "8000/8000 [==============================] - 6s 779us/step - loss: 0.0844 - acc: 0.9732 - val_loss: 0.6454 - val_acc: 0.8664\n",
            "Epoch 93/100\n",
            "8000/8000 [==============================] - 6s 788us/step - loss: 0.0827 - acc: 0.9737 - val_loss: 0.6449 - val_acc: 0.8683\n",
            "Epoch 94/100\n",
            "8000/8000 [==============================] - 6s 783us/step - loss: 0.0819 - acc: 0.9739 - val_loss: 0.6501 - val_acc: 0.8688\n",
            "Epoch 95/100\n",
            "8000/8000 [==============================] - 7s 823us/step - loss: 0.0799 - acc: 0.9747 - val_loss: 0.6512 - val_acc: 0.8692\n",
            "Epoch 96/100\n",
            "8000/8000 [==============================] - 6s 804us/step - loss: 0.0791 - acc: 0.9748 - val_loss: 0.6531 - val_acc: 0.8682\n",
            "Epoch 97/100\n",
            "8000/8000 [==============================] - 6s 779us/step - loss: 0.0774 - acc: 0.9750 - val_loss: 0.6595 - val_acc: 0.8685\n",
            "Epoch 98/100\n",
            "8000/8000 [==============================] - 6s 807us/step - loss: 0.0765 - acc: 0.9754 - val_loss: 0.6615 - val_acc: 0.8683\n",
            "Epoch 99/100\n",
            "8000/8000 [==============================] - 7s 856us/step - loss: 0.0754 - acc: 0.9756 - val_loss: 0.6649 - val_acc: 0.8677\n",
            "Epoch 100/100\n",
            "8000/8000 [==============================] - 6s 766us/step - loss: 0.0736 - acc: 0.9762 - val_loss: 0.6734 - val_acc: 0.8685\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AHZ-gS05OebS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Run training\n",
        "model_1 = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "model_1.compile(optimizer='rmsprop', loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "model_1.fit([encoder_input_data, decoder_input_data], decoder_target_data,\n",
        "          batch_size=batch_size,\n",
        "          epochs=10,\n",
        "          validation_split=0.2)\n",
        "# Save model\n",
        "model_1.save('s2s_1.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "90oqq9QHkKAt"
      },
      "source": [
        "## Inference mode\n",
        "1. Encode the input sentence and retrieve the initial decoder state\n",
        "2. Run one step of the decoder with this initial state and a \"start of sequence\" token as target. The output will be the next target character.\n",
        "3. Append the target character predicted and repeat."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "RTzw02ZaPCUs",
        "colab": {}
      },
      "source": [
        "# Next: inference mode (sampling).\n",
        "# Here's the drill:\n",
        "# 1) encode input and retrieve initial decoder state\n",
        "# 2) run one step of decoder with this initial state\n",
        "# and a \"start of sequence\" token as target.\n",
        "# Output will be the next target token\n",
        "# 3) Repeat with the current target token and current states\n",
        "\n",
        "# Define sampling models\n",
        "encoder_model = Model(encoder_inputs, encoder_states)\n",
        "\n",
        "decoder_state_input_h = Input(shape=(latent_dim,))\n",
        "decoder_state_input_c = Input(shape=(latent_dim,))\n",
        "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
        "decoder_outputs, state_h, state_c = decoder_lstm(\n",
        "    decoder_inputs, initial_state=decoder_states_inputs)\n",
        "decoder_states = [state_h, state_c]\n",
        "decoder_outputs = decoder_dense(decoder_outputs)\n",
        "decoder_model = Model(\n",
        "    [decoder_inputs] + decoder_states_inputs,\n",
        "    [decoder_outputs] + decoder_states)\n",
        "\n",
        "# Reverse-lookup token index to decode sequences back to\n",
        "# something readable.\n",
        "reverse_input_char_index = dict(\n",
        "    (i, char) for char, i in input_token_index.items())\n",
        "reverse_target_char_index = dict(\n",
        "    (i, char) for char, i in target_token_index.items())\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "dwQQ8UANPF3B",
        "colab": {}
      },
      "source": [
        "## Package the model into functions"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "D3NF9X-8PKmq",
        "colab": {}
      },
      "source": [
        "def decode_sequence(input_seq):\n",
        "    \"\"\"\n",
        "    Function to translate the inout sequence from English to French\n",
        "    \"\"\"\n",
        "    # Encode the input as state vectors.\n",
        "    states_value = encoder_model.predict(input_seq)\n",
        "\n",
        "    # Generate empty target sequence of length 1.\n",
        "    target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
        "    # Populate the first character of target sequence with the start character.\n",
        "    target_seq[0, 0, target_token_index['\\t']] = 1.\n",
        "\n",
        "    # Sampling loop for a batch of sequences\n",
        "    # (to simplify, here we assume a batch of size 1).\n",
        "    stop_condition = False\n",
        "    decoded_sentence = ''\n",
        "    while not stop_condition:\n",
        "        output_tokens, h, c = decoder_model.predict(\n",
        "            [target_seq] + states_value)\n",
        "\n",
        "        # Sample a token\n",
        "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
        "        sampled_char = reverse_target_char_index[sampled_token_index]\n",
        "        decoded_sentence += sampled_char\n",
        "\n",
        "        # Exit condition: either hit max length\n",
        "        # or find stop character.\n",
        "        if (sampled_char == '\\n' or\n",
        "           len(decoded_sentence) > max_decoder_seq_length):\n",
        "            stop_condition = True\n",
        "\n",
        "        # Update the target sequence (of length 1).\n",
        "        target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
        "        target_seq[0, 0, sampled_token_index] = 1.\n",
        "\n",
        "        # Update states\n",
        "        states_value = [h, c]\n",
        "\n",
        "    return decoded_sentence"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "d-bMv_jblA05",
        "colab": {}
      },
      "source": [
        "def convert_text_to_encoder_input(test_text):\n",
        "    \"\"\"\n",
        "    Convert raw text to encoder input\n",
        "    \"\"\"\n",
        "    test_input_data = np.zeros( (1, max_encoder_seq_length, num_encoder_tokens), dtype='float32')\n",
        "    for t, char in enumerate(test_text):\n",
        "        test_input_data[ 0,t, input_token_index[char]] = 1.0\n",
        "    test_input_data[0, t + 1:, input_token_index[' ']] = 1.0\n",
        "    \n",
        "    return test_input_data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TaZqSD7SC-fG",
        "colab_type": "text"
      },
      "source": [
        "## Model in action on part of training set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pDbdqga9C-fH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for seq_index in range(6,20,1):\n",
        "    # Take one sequence (part of the training set)\n",
        "    # for trying out decoding.\n",
        "    input_seq = encoder_input_data[seq_index: seq_index + 1]\n",
        "    decoded_sentence = decode_sequence(input_seq)\n",
        "    print('-')\n",
        "    print('Input sentence:', input_texts[seq_index])\n",
        "    print('Decoded sentence:', decoded_sentence)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aRMZFqd0C-fJ",
        "colab_type": "text"
      },
      "source": [
        "## Fun time!\n",
        "\n",
        "### Translation time\n",
        "### Health warning: this is not a production grade model or code (it will break)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "4Je09R_mYo1w",
        "colab": {}
      },
      "source": [
        "stop_condition = False\n",
        "while not stop_condition :\n",
        "  word_in_english =input()\n",
        "  if word_in_english != '**end':\n",
        "    input_to_model = convert_text_to_encoder_input(word_in_english)\n",
        "    word_in_french = decode_sequence(input_to_model)\n",
        "    print('Input sentence:', word_in_english)\n",
        "    print('Decoded sentence:', word_in_french )\n",
        "  else:\n",
        "    stop_condition = True"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}